{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UrkSsCyT5CoR"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObuqtNmF5CoW"
      },
      "source": [
        "**Module** is an abstract class which defines fundamental methods necessary for a training a neural network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-RImaVZ5CoW"
      },
      "outputs": [],
      "source": [
        "class Module(object):\n",
        "    \"\"\"\n",
        "    Basically, you can think of a module as of a something (black box) \n",
        "    which can process `input` data and produce `ouput` data.\n",
        "    This is like applying a function which is called `forward`: \n",
        "        \n",
        "        output = module.forward(input)\n",
        "    \n",
        "    The module should be able to perform a backward pass: to differentiate the `forward` function. \n",
        "    Moreover, it should be able to differentiate it if is a part of chain (chain rule).\n",
        "    The latter implies there is a gradient from previous step of a chain rule. \n",
        "    \n",
        "        input_grad = module.backward(input, output_grad)\n",
        "    \"\"\"\n",
        "    def __init__ (self):\n",
        "        self._output = None\n",
        "        self._input_grad = None\n",
        "        self.training = True\n",
        "    \n",
        "    def forward(self, input):\n",
        "        \"\"\"\n",
        "        Takes an input object, and computes the corresponding output of the module.\n",
        "        \"\"\"\n",
        "        self._output = self._compute_output(input)\n",
        "        return self._output\n",
        "\n",
        "    def backward(self, input, output_grad):\n",
        "        \"\"\"\n",
        "        Performs a backpropagation step through the module, with respect to the given input.\n",
        "        \n",
        "        This includes \n",
        "         - computing a gradient w.r.t. `input` (is needed for further backprop),\n",
        "         - computing a gradient w.r.t. parameters (to update parameters while optimizing).\n",
        "        \"\"\"\n",
        "        self._input_grad = self._compute_input_grad(input, output_grad)\n",
        "        self._update_parameters_grad(input, output_grad)\n",
        "        return self._input_grad\n",
        "    \n",
        "\n",
        "    def _compute_output(self, input):\n",
        "        \"\"\"\n",
        "        Computes the output using the current parameter set of the class and input.\n",
        "        This function returns the result which will be stored in the `_output` field.\n",
        "\n",
        "        Example: in case of identity operation:\n",
        "        \n",
        "        output = input \n",
        "        return output\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "        \n",
        "\n",
        "    def _compute_input_grad(self, input, output_grad):\n",
        "        \"\"\"\n",
        "        Returns the gradient of the module with respect to its own input. \n",
        "        The shape of the returned value is always the same as the shape of `input`.\n",
        "        \n",
        "        Example: in case of identity operation:\n",
        "        input_grad = output_grad\n",
        "        return input_grad\n",
        "        \"\"\"\n",
        "        \n",
        "        raise NotImplementedError\n",
        "    \n",
        "    def _update_parameters_grad(self, input, output_grad):\n",
        "        \"\"\"\n",
        "        Computing the gradient of the module with respect to its own parameters.\n",
        "        No need to override if module has no parameters (e.g. ReLU).\n",
        "        \"\"\"\n",
        "        pass\n",
        "    \n",
        "    def zero_grad(self): \n",
        "        \"\"\"\n",
        "        Zeroes `gradParams` variable if the module has params.\n",
        "        \"\"\"\n",
        "        pass\n",
        "        \n",
        "    def get_parameters(self):\n",
        "        \"\"\"\n",
        "        Returns a list with its parameters. \n",
        "        If the module does not have parameters return empty list. \n",
        "        \"\"\"\n",
        "        return []\n",
        "        \n",
        "    def get_parameters_grad(self):\n",
        "        \"\"\"\n",
        "        Returns a list with gradients with respect to its parameters. \n",
        "        If the module does not have parameters return empty list. \n",
        "        \"\"\"\n",
        "        return []\n",
        "    \n",
        "    def train(self):\n",
        "        \"\"\"\n",
        "        Sets training mode for the module.\n",
        "        Training and testing behaviour differs for Dropout, BatchNorm.\n",
        "        \"\"\"\n",
        "        self.training = True\n",
        "    \n",
        "    def evaluate(self):\n",
        "        \"\"\"\n",
        "        Sets evaluation mode for the module.\n",
        "        Training and testing behaviour differs for Dropout, BatchNorm.\n",
        "        \"\"\"\n",
        "        self.training = False\n",
        "    \n",
        "    def __repr__(self):\n",
        "        \"\"\"\n",
        "        Pretty printing. Should be overrided in every module if you want \n",
        "        to have readable description. \n",
        "        \"\"\"\n",
        "        return \"Module\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isbDKqDW5Col"
      },
      "source": [
        "# Layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybvPD8p05Coq"
      },
      "source": [
        "## 1. Batch normalization\n",
        "- input:   **`batch_size x n_feats`**\n",
        "- output: **`batch_size x n_feats`**\n",
        "\n",
        "The layer works as follows. While training (`self.training == True`) it transforms input as $$y = \\frac{x - \\mu}  {\\sqrt{\\sigma + \\epsilon}}$$\n",
        "where $\\mu$ and $\\sigma$ — mean and variance of feature values in **batch** and $\\epsilon$ is just a small number for numericall stability. Also during training, layer maintains exponential moving average values for mean and variance: \n",
        "```\n",
        "    self.moving_mean = self.moving_mean * alpha + batch_mean * (1 - alpha)\n",
        "    self.moving_variance = self.moving_variance * alpha + batch_variance * (1 - alpha)\n",
        "```\n",
        "During testing (`self.training == False`) the layer normalizes input using moving_mean and moving_variance. \n",
        "\n",
        "Note that decomposition of batch normalization on normalization itself and channelwise scaling here is just a common **implementation** choice. In general \"batch normalization\" always assumes normalization + scaling.\n",
        "\n",
        "### Backpropagation\n",
        "Suppose $X$ has shape $n×m$ where $n$ is number of samples in a batch and $m$ is number of nodes in the layer , $J$ is matrix of ones (default with shape $n×n$). Then the centered $X$, $\\: X_c$, is: \n",
        "\n",
        "$$X_c = (X - \\frac{1}{n} J X)$$ \n",
        "\n",
        "The broadcasted standard deviation matrix of $X$, $\\: X_s$, is:\n",
        "\n",
        "$$X_s = (\\frac{1}{n} J X_c^{\\circ^2} + \\epsilon J)^{\\circ^{\\frac{1}{2}}}$$\n",
        "where $\\circ$ denotes element-wise power. $\\epsilon$  is a tiny scalar to avoid division by zero.\n",
        "\n",
        "Then the normalized $X$, $\\: X_n$ is:\n",
        "\n",
        "$$X_n = X_c \\odot X_s^{\\circ^{-1}}$$\n",
        "\n",
        "where $\\odot$ is element-wise product.\n",
        "\n",
        "For column vectors $\\gamma$ and $\\beta$, the transformed normalized $X$,  the output of batch normalization $\\hat{X}$ is:\n",
        "\n",
        "$$\\hat{X} = X_n \\odot (1_n \\gamma^T) + 1_n \\beta^T$$\n",
        "\n",
        "where $1_n$ is column vector of ones with proper shape.\n",
        "\n",
        "Several trace tricks:\n",
        "- $a)\\: X : Y = tr(X^T Y)$\n",
        "- $b)\\: x = tr(x)$\n",
        "- $c) x = x^T$\n",
        "- $d)\\: tr(aX + bY) = a tr(X) + b tr(Y)$\n",
        "- $e)\\: tr(XY) = tr(YX)$\n",
        "- $f)\\: tr(X^T) = tr(X)$\n",
        "- $g)\\: tr(X(Y \\odot Z)) = tr((X \\odot Y^T)Z)$\n",
        "- $h)\\: (xy^T) \\odot Z = D_x Z D_y; \\: D_x$ denotes diagonal matrix with $x$ on diagonal\n",
        "- $i)\\: X(Y \\odot (1_n z^T)) = (X Y) \\odot (1_n z^T)$\n",
        "- $j)\\: ((z 1_n^T) \\odot X) Y = (z 1_n^T) \\odot (X Y)$\n",
        "\n",
        "$\\mathcal{L} \\:$ denotes output scalar value of a loss function\n",
        "\n",
        "Calculating Frechet derivative of $\\mathcal{L}$ with respect to $\\beta$:\n",
        "\n",
        "$\n",
        "D_{\\beta_0}[\\mathcal{L}](h) \\\\ = tr(\\nabla_{\\hat{X}} \\mathcal{L}^T \\cdot D_{\\beta_0}[1_n \\beta^T](h)) \\\\ = tr(\\nabla_{\\hat{X}} \\mathcal{L}^T \\cdot 1_n h^T) \\\\ = tr(1_n^T \\cdot \\nabla_{\\hat{X}} \\mathcal{L} \\cdot h)\n",
        "$\n",
        "\n",
        "$\n",
        "\\nabla_{\\beta} \\mathcal{L} = (1_n^T \\cdot \\nabla_{\\hat{X}} \\mathcal{L})^T \n",
        "$\n",
        "\n",
        "Calculating Frechet derivative of $\\mathcal{L}$ with respect to $\\gamma$:\n",
        "\n",
        "$\n",
        "D_{\\gamma_0}[\\mathcal{L}](h) \\\\ = tr(\\nabla_{\\hat{X}} \\mathcal{L}^T \\cdot D_{\\gamma_0}[X_n \\odot (1_n \\gamma^T)](h)) \\\\ = tr(\\nabla_{\\hat{X}} \\mathcal{L}^T \\cdot (X_n \\odot (1_n h^T))) \\\\ = tr((\\nabla_{\\hat{X}} \\mathcal{L}^T \\odot X_n^T) \\cdot (1_n h^T)) \\\\ = tr(1_n^T \\cdot (\\nabla_{\\hat{X}} \\mathcal{L} \\odot X_n) \\cdot h)\n",
        "$\n",
        "\n",
        "$\n",
        "\\nabla_{\\gamma} \\mathcal{L} = (1_n^T \\cdot (\\nabla_{\\hat{X}} \\mathcal{L} \\odot X_n))^T\n",
        "$\n",
        "\n",
        "Calculating Frechet derivative of $\\mathcal{L}$ with respect to $X_n$:\n",
        "\n",
        "$\n",
        "D_{X_{n_0}}[\\mathcal{L}](H) \\\\ =  tr(\\nabla_{\\hat{X}} \\mathcal{L}^T \\cdot D_{X_{n_0}}[X_n \\odot (1_n \\gamma^T)](H)) \\\\ = tr(\\nabla_{\\hat{X}} \\mathcal{L}^T \\cdot (H \\odot (1_n \\gamma^T))) \\\\ = tr((\\nabla_{\\hat{X}} \\mathcal{L}^T \\odot (1_n \\gamma^T)^T) \\cdot H) \\\\ = tr((\\nabla_{\\hat{X}} \\mathcal{L} \\odot (1_n \\gamma^T))^T \\cdot H)\n",
        "$\n",
        "\n",
        "$\n",
        "\\nabla_{X_n} \\mathcal{L} = \\nabla_{\\hat{X}} \\mathcal{L} \\odot (1_n \\gamma^T)\n",
        "$\n",
        "\n",
        "Calculating Frechet derivative of $X_n$ with respect to $X_c$:\n",
        "\n",
        "$\n",
        "D_{X_{c_0}}[X_n](H) \\\\ = D_{X_{c_0}}[X_c \\odot X_s^{\\circ^{-1}}](H) \\\\ = D_{X_{c_0}}[X_c](H) \\odot X_{s_0}^{\\circ^{-1}} + X_{c_0} \\odot D_{X_{c_0}}[X_s^{\\circ^{-1}}](H) \\\\ = H \\odot X_{s_0}^{\\circ^{-1}} - X_{c_0} \\odot X_{s_0}^{\\circ^{-2}} \\odot D_{X_{c_0}}[(\\frac{1}{n} J X_c^{\\circ^2} + \\epsilon J)^{\\circ^{\\frac{1}{2}}}](H) \\\\ = H \\odot X_{s_0}^{\\circ^{-1}} - X_{c_0} \\odot X_{s_0}^{\\circ^{-2}} \\odot (\\frac{1}{2} X_{s_0}^{\\circ^{-1}} \\odot D_{X_{c_0}}[\\frac{1}{n} J X_c^{\\circ^2} + \\epsilon J](H)) \\\\ = H \\odot X_{s_0}^{\\circ^{-1}} - X_{c_0} \\odot X_{s_0}^{\\circ^{-2}} \\odot (\\frac{1}{2} X_{s_0}^{\\circ^{-1}} (2 \\frac{1}{n} J \\cdot (X_{c_0} \\odot D_{X_{c_0}}[X_c](H)))) \\\\ = H \\odot X_{s_0}^{\\circ^{-1}} - \\frac{1}{n} X_{n_0} \\odot X_{s_0}^{\\circ^{-1}} \\odot (X_{s_0}^{\\circ^{-1}} \\odot J \\cdot (X_{c_0} \\odot H)) \\\\ = H \\odot X_{s_0}^{\\circ^{-1}} - \\frac{1}{n} X_{n_0} \\odot X_{s_0}^{\\circ^{-1}} \\odot J \\cdot (X_{c_0} \\odot X_{s_0}^{\\circ^{-1}} \\odot H) \\\\ = H \\odot X_{s_0}^{\\circ^{-1}} - \\frac{1}{n} X_{n_0} \\odot X_{s_0}^{\\circ^{-1}} \\odot J \\cdot (X_{n_0} \\odot H) \\\\ = X_{s_0}^{\\circ^{-1}} \\odot (H - \\frac{1}{n} X_{n_0} \\odot (J \\cdot (X_{n_0} \\odot H)))\n",
        "$\n",
        "\n",
        "Now we can calculate derivative of $\\mathcal{L}$ with respect to $X_c$:\n",
        "\n",
        "$\n",
        "D_{X_{c_0}}[\\mathcal{L}](H) \\\\ =  tr(\\nabla_{X_n} \\mathcal{L}^T \\cdot D_{X_{c_0}}[X_n](H)) \\\\ = tr(\\nabla_{X_n} \\mathcal{L}^T \\cdot (X_{s_0}^{\\circ^{-1}} \\odot (H - \\frac{1}{n} X_{n_0} \\odot (J \\cdot (X_{n_0} \\odot H))))) \\\\ = tr(\\nabla_{X_n} \\mathcal{L}^T \\cdot (X_{s_0}^{\\circ^{-1}} \\odot H)) - \\frac{1}{n} tr(\\nabla_{X_n} \\mathcal{L}^T \\cdot (X_{s_0}^{\\circ^{-1}} \\odot X_{n_0} \\odot (J \\cdot (X_{n_0} \\odot H)))) \\\\ = tr((\\nabla_{X_n} \\mathcal{L} \\odot X_{s_0}^{\\circ^{-1}})^T \\cdot H) - \\frac{1}{n} tr((\\nabla_{X_n} \\mathcal{L} \\odot X_{s_0}^{\\circ^{-1}} \\odot X_{n_0})^T \\cdot (J \\cdot (X_{n_0} \\odot H))) \\\\ = tr((\\nabla_{X_n} \\mathcal{L} \\odot X_{s_0}^{\\circ^{-1}})^T \\cdot H) - \\frac{1}{n} tr(J \\cdot (\\nabla_{X_n} \\mathcal{L} \\odot X_{s_0}^{\\circ^{-1}} \\odot X_{n_0})^T \\cdot (X_{n_0} \\odot H)) \\\\ = tr((\\nabla_{X_n} \\mathcal{L} \\odot X_{s_0}^{\\circ^{-1}})^T \\cdot H) - \\frac{1}{n} tr((X_{s_0}^{\\circ^{-1}} \\odot J (\\nabla_{X_n} \\mathcal{L} \\odot X_{n_0}))^T \\cdot (X_{n_0} \\odot H)) \\\\ = tr((\\nabla_{X_n} \\mathcal{L} \\odot X_{s_0}^{\\circ^{-1}})^T \\cdot H) - \\frac{1}{n} tr((X_{s_0}^{\\circ^{-1}} \\odot X_{n_0} \\odot J (\\nabla_{X_n} \\mathcal{L} \\odot X_{n_0}))^T \\cdot H) \\\\ = tr((X_{s_0}^{\\circ^{-1}} \\odot (\\nabla_{X_n} \\mathcal{L} - \\frac{1}{n} X_{n_0} \\odot J (X_{n_0} \\odot \\nabla_{X_n} \\mathcal{L})))^T \\cdot H)\n",
        "$\n",
        "\n",
        "$\n",
        "\\nabla_{X_c} \\mathcal{L} = X_{s_0}^{\\circ^{-1}} \\odot (\\nabla_{X_n} \\mathcal{L} - \\frac{1}{n} X_{n_0} \\odot J (X_{n_0} \\odot \\nabla_{X_n} \\mathcal{L}))\n",
        "$\n",
        "\n",
        "Calculating Frechet derivative of $\\mathcal{L}$ with respect to $X$:\n",
        "\n",
        "$\n",
        "D_{X_0}[\\mathcal{L}](H) \\\\ =  tr(\\nabla_{X_c} \\mathcal{L}^T \\cdot D_{X_0}[X_c](H)) \\\\ = tr(\\nabla_{X_c} \\mathcal{L}^T \\cdot D_{X_0}[X - \\frac{1}{n} J X](H)) \\\\ = tr(\\nabla_{X_c} \\mathcal{L}^T \\cdot (I - \\frac{1}{n} J) \\cdot H) \\\\ = tr(((I - \\frac{1}{n} J) \\cdot \\nabla_{X_c} \\mathcal{L})^T \\cdot H)\n",
        "$\n",
        "\n",
        "$\n",
        "\\nabla_{X} \\mathcal{L} = (I - \\frac{1}{n} J) \\cdot \\nabla_{X_c} \\mathcal{L} = (I - \\frac{1}{n} J) \\cdot (\\nabla_{X_n} \\mathcal{L} - \\frac{1}{n} X_{n_0} \\odot J (X_{n_0} \\odot \\nabla_{X_n} \\mathcal{L})) \\odot X_{s_0}^{\\circ^{-1}}\n",
        "$\n",
        "\n",
        "Notice that:\n",
        "\n",
        "$J \\cdot ((J(X_{n_0} \\odot \\nabla_{X_n} \\mathcal{L})) \\odot X_{n_0}) = (J(X_{n_0} \\odot \\nabla_{X_n} \\mathcal{L})) \\odot (J \\cdot X_{n_0}) = 0 \\:$, as $J \\cdot X_{n_0} = 0$\n",
        "\n",
        "So:\n",
        "\n",
        "$\n",
        "\\nabla_{X} \\mathcal{L} \\\\ = X_{s_0}^{\\circ^{-1}} \\odot ((I - \\frac{1}{n} J) \\cdot \\nabla_{X_n} \\mathcal{L} - \\frac{1}{n} X_{n_0} \\odot (J (X_{n_0} \\odot \\nabla_{X_n} \\mathcal{L}))) \\\\ = X_{s_0}^{\\circ^{-1}} \\odot (((I - \\frac{1}{n} J) (\\nabla_{\\hat{X}} \\mathcal{L} \\odot (1_n \\gamma^T)) - \\frac{1}{n} X_{n_0} \\odot (J (X_{n_0} \\odot (\\nabla_{\\hat{X}} \\mathcal{L} \\odot (1_n \\cdot \\gamma^T)))))) \\\\ = X_{s_0}^{\\circ^{-1}} \\odot (1_n \\gamma^T) \\odot ((I - \\frac{1}{n} J) \\nabla_{\\hat{X}} \\mathcal{L} - \\frac{1}{n} X_{n_0} \\odot (J (X_{n_0} \\odot \\nabla_{\\hat{X}} \\mathcal{L})))\n",
        "$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQRFBjo55Cos"
      },
      "outputs": [],
      "source": [
        "class BatchNormalization(Module):\n",
        "    def __init__(self, alpha=0.):\n",
        "        super(BatchNormalization, self).__init__()\n",
        "        self.EPS = 1e-3\n",
        "        self.alpha = alpha\n",
        "        self.moving_mean = 0.\n",
        "        self.moving_variance = 1.\n",
        "\n",
        "    def _compute_output(self, input):\n",
        "        mean = input.mean(axis=0)\n",
        "        variance = input.var(axis=0)\n",
        "        if self.training:\n",
        "            self.moving_mean = self.moving_mean * self.alpha + mean * (1 - self.alpha)\n",
        "            self.moving_variance = self.moving_variance * self.alpha + variance * (1 - self.alpha)\n",
        "        else:\n",
        "            mean = self.moving_mean\n",
        "            variance = self.moving_variance\n",
        "\n",
        "        output = (input - mean) / np.sqrt(variance + self.EPS)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def _compute_input_grad(self, input, output_grad):\n",
        "        n = output_grad.shape[0]\n",
        "        input_centered = input - input.mean(axis=0, keepdims=True)\n",
        "        input_std = np.sqrt((input_centered ** 2).mean(axis=0, keepdims=True) + self.EPS)\n",
        "        input_normalized = input_centered / input_std\n",
        "        \n",
        "        output_grad_centered = output_grad - output_grad.mean(axis=0, keepdims=True)\n",
        "\n",
        "        input_normalized_x_output_grad = np.sum(input_normalized * output_grad,  axis=0, keepdims=True)\n",
        "        \n",
        "        grad_input = (output_grad_centered - input_normalized * input_normalized_x_output_grad / n) / input_std\n",
        "        return grad_input\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"BatchNormalization\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iU8LwKVy5Cot"
      },
      "outputs": [],
      "source": [
        "class ChannelwiseScaling(Module):\n",
        "    \"\"\"\n",
        "       Implements linear transform of input y = \\gamma * x + \\beta\n",
        "       where \\gamma, \\beta - learnable vectors of length x.shape[-1]\n",
        "    \"\"\"\n",
        "    def __init__(self, n_out):\n",
        "        super(ChannelwiseScaling, self).__init__()\n",
        "\n",
        "        stdv = 1./np.sqrt(n_out)\n",
        "        self.gamma = np.random.uniform(-stdv, stdv, size=n_out)\n",
        "        self.beta = np.random.uniform(-stdv, stdv, size=n_out)\n",
        "        \n",
        "        self.gradGamma = np.zeros_like(self.gamma)\n",
        "        self.gradBeta = np.zeros_like(self.beta)\n",
        "\n",
        "    def _compute_output(self, input):\n",
        "        output = input * self.gamma + self.beta\n",
        "        return output\n",
        "        \n",
        "    def _compute_input_grad(self, input, output_grad):\n",
        "        grad_input = output_grad * self.gamma\n",
        "        return grad_input\n",
        "    \n",
        "    def _update_parameters_grad(self, input, output_grad):\n",
        "        self.gradBeta = np.sum(output_grad, axis=0)\n",
        "        self.gradGamma = np.sum(output_grad*input, axis=0)\n",
        "    \n",
        "    def zero_grad(self):\n",
        "        self.gradGamma.fill(0)\n",
        "        self.gradBeta.fill(0)\n",
        "        \n",
        "    def get_parameters(self):\n",
        "        return [self.gamma, self.beta]\n",
        "    \n",
        "    def get_parameters_grad(self):\n",
        "        return [self.gradGamma, self.gradBeta]\n",
        "    \n",
        "    def __repr__(self):\n",
        "        return \"ChannelwiseScaling\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9Wcfkiu5Cov"
      },
      "source": [
        "## 2. Dropout\n",
        "Implementation of [**dropout**](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf). The idea and implementation is really simple: just multimply the input by $Bernoulli(p)$ mask. Here $p$ is probability of an element to be zeroed.\n",
        "\n",
        "This has proven to be an effective technique for regularization and preventing the co-adaptation of neurons.\n",
        "\n",
        "While training (`self.training == True`) it samples a mask on each iteration (for every batch), zero out elements and multiply elements by $1 / (1 - p)$. The latter is needed for keeping mean values of features close to mean values which will be in test mode. When testing this module uses identity transform i.e. `output = input`.\n",
        "\n",
        "- input:   **`batch_size x n_feats`**\n",
        "- output: **`batch_size x n_feats`**\n",
        "\n",
        "### Backpropagation\n",
        "\n",
        "$\n",
        "\\hat{X} = X \\odot M\n",
        "$\n",
        "\n",
        "$\n",
        "D_{X_0}[\\mathcal{L}](H) \\\\ =  tr(\\nabla_{\\hat{X}} \\mathcal{L}^T \\cdot D_{X_0}[\\hat{X}](H)) \\\\ = tr(\\nabla_{\\hat{X}} \\mathcal{L}^T \\cdot D_{X_0}[X \\odot M](H)) \\\\ = tr(\\nabla_{\\hat{X}} \\mathcal{L}^T \\cdot (M \\odot H)) \\\\ = tr((\\nabla_{\\hat{X}} \\mathcal{L}^T \\odot M^T) \\cdot H) \\\\ = tr((\\nabla_{\\hat{X}} \\mathcal{L} \\odot M)^T \\cdot H))\n",
        "$\n",
        "\n",
        "$\n",
        "\\nabla_{X} \\mathcal{L} = \\nabla_{\\hat{X}} \\mathcal{L} \\odot M\n",
        "$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AxnN67MZ5Cow"
      },
      "outputs": [],
      "source": [
        "class Dropout(Module):\n",
        "  \n",
        "    def __init__(self, p=0.5):\n",
        "        super(Dropout, self).__init__()\n",
        "        \n",
        "        self.p = p\n",
        "        self.mask = []\n",
        "        \n",
        "    def _compute_output(self, input):\n",
        "        n = input.shape[0]\n",
        "\n",
        "        if self.training:\n",
        "            self.mask = np.random.binomial(1, 1 - self.p, input.shape) / (1 - self.p)\n",
        "        else:\n",
        "            self.mask = np.ones_like(input)\n",
        "        output = input * self.mask\n",
        "        return output\n",
        "    \n",
        "    def _compute_input_grad(self, input, output_grad):\n",
        "        grad_input = output_grad * self.mask\n",
        "        return grad_input\n",
        "        \n",
        "    def __repr__(self):\n",
        "        return \"Dropout\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDVJ65FU_qVd"
      },
      "source": [
        "## 3. Conv2d\n",
        "Implementation of something like pytorch Conv2d layer with `stride=1` and zero-padding outside of image using `scipy.signal.correlate` function.\n",
        "\n",
        "* input: `batch_size x in_channels x h x w`\n",
        "* output: `batch_size x out_channels x h x w`\n",
        "\n",
        "### Forward\n",
        "\n",
        "$$\n",
        "\\begin{cases}\n",
        "  Y_1 = B_1 + X_1 \\star K_{11} + ... + X_n \\star K_{1n} \\\\\n",
        "  Y_2 = B_2 + X_1 \\star K_{21} + ... + X_n \\star K_{2n} \\\\\n",
        "  ... \\\\\n",
        "  Y_d = B_d + X_1 \\star K_{d1} + ... + X_n \\star K_{dn} \\\\\n",
        "\\end{cases}\n",
        "$$\n",
        "where $\\star$ denotes cross-correlation operator.\n",
        "\n",
        "\n",
        "### Backpropagation\n",
        "\n",
        "$$\n",
        "\\begin{cases}\n",
        "  \\dfrac{\\partial \\mathcal{L}}{\\partial K_{ij}} = X_j \\star \\dfrac{\\partial \\mathcal{L}}{\\partial Y_i} \\\\\n",
        "  \\dfrac{\\partial \\mathcal{L}}{\\partial B_i} = \\dfrac{\\partial \\mathcal{L}}{\\partial Y_i} \\\\\n",
        "  \\dfrac{\\partial \\mathcal{L}}{\\partial X_j} = \\sum_{i=1}^{d} \\dfrac{\\partial \\mathcal{L}}{\\partial Y_i} \\ast K_{ij}\n",
        "\\end{cases}\n",
        "$$\n",
        "where $\\ast$ denotes full-convolution operator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZuhBnJ6w_qVe"
      },
      "outputs": [],
      "source": [
        "import scipy as sp\n",
        "import scipy.signal\n",
        "import skimage\n",
        "\n",
        "class Conv2d(Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size):\n",
        "        super(Conv2d, self).__init__()\n",
        "        assert kernel_size % 2 == 1, kernel_size\n",
        "       \n",
        "        stdv = 1./np.sqrt(in_channels)\n",
        "        self.W = np.random.uniform(-stdv, stdv, size = (out_channels, in_channels, kernel_size, kernel_size))\n",
        "        self.b = np.random.uniform(-stdv, stdv, size=(out_channels,))\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        \n",
        "        self.gradW = np.zeros_like(self.W)\n",
        "        self.gradb = np.zeros_like(self.b)\n",
        "        \n",
        "    def _compute_output(self, input):\n",
        "        pad_size = self.kernel_size // 2\n",
        "        n, c, w, h = input.shape\n",
        "        input = np.pad(input, ((0, 0), (0, 0), (pad_size, pad_size), (pad_size, pad_size)), \n",
        "                       'constant', constant_values=0)\n",
        "        \n",
        "        self._output = np.zeros((n, self.out_channels, w, h))\n",
        "        self._output += np.repeat(self.b[np.newaxis, :, np.newaxis, np.newaxis], n, axis=0)\n",
        "\n",
        "        for sample_index in range(n):\n",
        "            for out_channel_index in range(self.out_channels):\n",
        "                for in_channel_index in range(self.in_channels):\n",
        "                    self._output[sample_index][out_channel_index] += \\\n",
        "                        scipy.signal.correlate2d(input[sample_index][in_channel_index], \n",
        "                                                 self.W[out_channel_index, in_channel_index], \"valid\")\n",
        "\n",
        "        return self._output\n",
        "    \n",
        "    def _compute_input_grad(self, input, gradOutput):\n",
        "        pad_size = self.kernel_size // 2\n",
        "        n, c, w, h = input.shape\n",
        "        self._input_grad = np.zeros_like(input)\n",
        "        gradOutput = np.pad(gradOutput, ((0, 0), (0, 0), (pad_size, pad_size), (pad_size, pad_size)), \n",
        "                            'constant', constant_values=0)\n",
        "\n",
        "        for sample_index in range(n):\n",
        "            for out_channel_index in range(self.out_channels):\n",
        "                for in_channel_index in range(self.in_channels):\n",
        "                    self._input_grad[sample_index][in_channel_index] += \\\n",
        "                        scipy.signal.convolve2d(gradOutput[sample_index][out_channel_index], \n",
        "                                                self.W[out_channel_index, in_channel_index], \"valid\")\n",
        "\n",
        "        return self._input_grad\n",
        "    \n",
        "    def accGradParameters(self, input, gradOutput):\n",
        "        pad_size = self.kernel_size // 2\n",
        "        n, c, w, h = input.shape\n",
        "        input = np.pad(input, ((0, 0), (0, 0), (pad_size, pad_size), (pad_size, pad_size)), \n",
        "                       'constant', constant_values=0)\n",
        "\n",
        "        for sample_index in range(n):\n",
        "            for out_channel_index in range(self.out_channels):\n",
        "                for in_channel_index in range(self.in_channels):\n",
        "                    self.gradW[out_channel_index, in_channel_index] += \\\n",
        "                        scipy.signal.correlate2d(input[sample_index][in_channel_index], \n",
        "                                                 gradOutput[sample_index][out_channel_index], \"valid\")\n",
        "\n",
        "        self.gradb = np.sum(gradOutput, axis=(0, 2, 3))\n",
        "        pass\n",
        "    \n",
        "    def zeroGradParameters(self):\n",
        "        self.gradW.fill(0)\n",
        "        self.gradb.fill(0)\n",
        "        \n",
        "    def getParameters(self):\n",
        "        return [self.W, self.b]\n",
        "    \n",
        "    def getGradParameters(self):\n",
        "        return [self.gradW, self.gradb]\n",
        "    \n",
        "    def __repr__(self):\n",
        "        s = self.W.shape\n",
        "        q = 'Conv2d %d -> %d' %(s[1],s[0])\n",
        "        return q"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "f98f21f0b58c314391d9edda6a890b43799e7bbdcfa23cfcf4ab03be958beb23"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}